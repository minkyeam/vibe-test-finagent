from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import yfinance as yf
import os
import asyncio
import requests as _requests
from datetime import datetime
from typing import Optional, List
import time
import pytz
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.output_parsers import StrOutputParser
from prompts import MACRO_ANALYSIS_PROMPT
import json

# Load environment variables
load_dotenv()

# yfinance ì„¸ì…˜ ì„¤ì • (í´ë¼ìš°ë“œ í™˜ê²½ ì°¨ë‹¨ ë°©ì§€)
import requests as req_lib
_session = req_lib.Session()
_session.headers.update({'User-Agent': 'Mozilla/5.0 FinAgent/1.0'})

app = FastAPI()

# CORS ì„¤ì • (ë³„ë„ ë„ë©”ì¸ ë°°í¬ ì‹œ í•„ìˆ˜)
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” Vercel ì£¼ì†Œë§Œ í—ˆìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â”€â”€ In-memory cache for market data (60s TTL) â”€â”€
_market_cache: dict = {"data": None, "signals": None, "fetched_at": 0}
CACHE_TTL = 60  # seconds

# â”€â”€ In-memory cache for FRED liquidity data (6h TTL) â”€â”€
_liquidity_cache: dict = {"data": None, "fetched_at": 0}
LIQUIDITY_CACHE_TTL = 6 * 3600  # 6ì‹œê°„ (FREDëŠ” ì¼/ì£¼ ë‹¨ìœ„ ì—…ë°ì´íŠ¸)

# â”€â”€ RAG: Load institutional reports once at startup â”€â”€
_institutional_context: str = "No specific reports available."
try:
    _report_path = os.path.join(os.path.dirname(__file__), "data", "institutional_reports.json")
    if os.path.exists(_report_path):
        with open(_report_path, "r", encoding="utf-8") as _f:
            _report_data = json.load(_f)
            _institutional_context = "\n\n".join([
                f"[{r['institution']} - {r['title']}]\nSummary: {r['summary']}\nDetails: {r['content']}"
                for r in _report_data.get("reports", [])
            ])
        print(f"[RAG] Loaded {len(_report_data.get('reports', []))} institutional reports.")
except Exception as _rag_e:
    print(f"[RAG] Load error: {_rag_e}")


class MarketData(BaseModel):
    symbol: str
    price: float
    change_percent: float
    name: str


@app.get("/")
def read_root():
    return {"message": "Financial Macro Agent API is running"}


def fetch_market_data_internal():
    symbols = {
        "KRW=X": "USD/KRW",
        "^KS11": "KOSPI",
        "^KQ11": "KOSDAQ",
        "^TNX": "US 10Y Bond",
        "^IRX": "US 13W Bond",
        "CL=F": "WTI Oil",
        "GC=F": "Gold",
        "BTC-USD": "Bitcoin",
        "ETH-USD": "Ethereum",
        "005930.KS": "Samsung Elec",
        "DX-Y.NYB": "Dollar Index",
    }

    data = []
    signals = []

    try:
        # yf.downloadì— ì„¸ì…˜ ì „ë‹¬ ë° ë²Œí¬ íŒ¨ì¹˜
        ticker_list = list(symbols.keys())
        df = yf.download(ticker_list, period="2d", interval="1d", group_by='ticker')
        
        for symbol, name in symbols.items():
            try:
                if symbol not in df.columns.levels[0]: continue
                hist = df[symbol]
                if hist.empty or len(hist) < 1: continue
                
                current = hist['Close'].iloc[-1]
                prev = hist['Close'].iloc[-2] if len(hist) > 1 else current
                change = ((current - prev) / prev) * 100 if prev != 0 else 0
                
                data.append({
                    "symbol": symbol,
                    "name": name,
                    "price": round(float(current), 2),
                    "change_percent": round(float(change), 2)
                })
            except Exception:
                continue
    except Exception as e:
        print(f"Error in batch fetching: {e}")

    # Yield Curve signal
    try:
        us10y = next((x for x in data if x['symbol'] == '^TNX'), None)
        us13w = next((x for x in data if x['symbol'] == '^IRX'), None)
        if us10y and us13w:
            spread = us10y['price'] - us13w['price']
            signals.append(f"Yield Curve (10Y-13W): {spread:.2f}bp ({'Inverted!' if spread < 0 else 'Normal'})")
    except Exception:
        pass

    return {"data": data, "signals": signals}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# â—€ ì—°ì¤€ / ì¬ë¬´ë¶€ ìœ ë™ì„± ì§€í‘œ (FRED ë¬´ë£Œ ê³µê°œ API)    #
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

FRED_LIQUIDITY_SERIES = {
    # ì—°ì¤€ ëŒ€ì°¨ëŒ€ì¡°í‘œ (QT/QE êµ¬ë¶„ í•µì‹¬) â€” ë‹¨ìœ„: ë°±ë§Œë‹¬ëŸ¬ â†’ 10ì–µë‹¬ëŸ¬($B)ë¡œ ë³€í™˜
    "WALCL":     {"name": "Fed Balance Sheet",         "unit": "$B",  "scale": 1e-3,  "desc": "ì—°ì¤€ ì´ìì‚° â€” QT/QE íŒë‹¨ í•µì‹¬ ì§€í‘œ"},
    # ON RRP â€” ë‹¨ê¸° ìœ ë™ì„± í†µì œ í†µë¡œ â€” ë‹¨ìœ„: 10ì–µë‹¬ëŸ¬($B)
    "RRPONTSYD": {"name": "ON RRP Balance",            "unit": "$B",  "scale": 1.0,  "desc": "ì—°ì¤€ ì—­ë ˆí¬ ì”ê³  (ë‹¨ê¸° ìœ ë™ì„± í¡ìˆ˜ëŸ‰)"},
    # TGA â€” ì¬ë¬´ë¶€ ì¼ë°˜ê³„ì¢Œ â€” ë‹¨ìœ„: ë°±ë§Œë‹¬ëŸ¬($M)
    "WTREGEN":   {"name": "Treasury General Account",  "unit": "$M",  "scale": 1.0,  "desc": "ì¬ë¬´ë¶€ TGA ì”ì•¡ â€” êµ­ì±„ ë°œí–‰Â·ìƒí™˜ ì‹œ ìœ ë™ì„± ì˜í–¥"},
    # SOFR â€” LIBOR ëŒ€ì²´ ê¸°ì¤€ê¸ˆë¦¬ â€” ë‹¨ìœ„: %
    "SOFR":      {"name": "SOFR Rate",                 "unit": "%",   "scale": 1.0,  "desc": "ë‹¨ê¸° ë‹´ë³´ë¶€ ê¸°ì¤€ê¸ˆë¦¬ (LIBOR ëŒ€ì²´)"},
    # ì‹¤íš¨ ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬ â€” ë‹¨ìœ„: %
    "DFF":       {"name": "Effective Fed Funds Rate",  "unit": "%",   "scale": 1.0,  "desc": "ì‹¤íš¨ ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬ (FOMC ì •ì±… ë°˜ì˜)"},
    # M2 í†µí™”ëŸ‰ â€” ë‹¨ìœ„: 10ì–µë‹¬ëŸ¬($B)
    "M2SL":      {"name": "M2 Money Supply",           "unit": "$B",  "scale": 1.0,  "desc": "M2 ê´‘ì˜ í†µí™”ëŸ‰ (ì‹œì¤‘ ìœ ë™ì„± ì´ëŸ‰)"},
}


def fetch_fred_series(series_id: str):
    """FRED ê³µê°œ CSV ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìµœì‹ ê°’ 1ê°œ ë°˜í™˜. API í‚¤ ë¶ˆí•„ìš”."""
    try:
        url = f"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}"
        resp = _requests.get(url, timeout=10,
                             headers={"User-Agent": "Mozilla/5.0 FinAgent/1.0"})
        resp.raise_for_status()
        lines = [l for l in resp.text.strip().splitlines() if l and not l.startswith("DATE")]
        last_line = lines[-1]  # ìµœì‹  ë°ì´í„°
        date_str, value_str = last_line.split(",")
        if value_str.strip() in ("", "."):  # ëˆ„ë½ê°’
            # ëˆ„ë½ê°’ì´ë©´ ì´ì „ ë°ì´í„° ì°¾ê¸°
            for line in reversed(lines):
                d, v = line.split(",")
                if v.strip() not in ("", "."):
                    return float(v.strip()), d.strip()
            return None, None
        return float(value_str.strip()), date_str.strip()
    except Exception as e:
        print(f"[FRED] Error fetching {series_id}: {e}")
        return None, None


def fetch_liquidity_data() -> List[dict]:
    """FRED 6ê°œ ì§€í‘œ íŒ¨ì¹˜ íš¨ì¹˜."""
    results = []
    for series_id, meta in FRED_LIQUIDITY_SERIES.items():
        value, date = fetch_fred_series(series_id)
        if value is not None:
            scaled = value * meta["scale"]
            results.append({
                "series": series_id,
                "name": meta["name"],
                "value": round(scaled, 2),
                "unit": meta["unit"],
                "date": date,
                "desc": meta["desc"],
            })
            print(f"[FRED] {series_id}: {scaled:.2f} {meta['unit']} ({date})")
        else:
            print(f"[FRED] {series_id}: N/A")
    return results


def _get_cached_liquidity() -> List[dict]:
    """FRED ìœ ë™ì„± ë°ì´í„° ì¼€ì‹œ (6h TTL)."""
    global _liquidity_cache
    now = time.time()
    if _liquidity_cache["data"] is not None and (now - _liquidity_cache["fetched_at"]) < LIQUIDITY_CACHE_TTL:
        print(f"[Liquidity Cache HIT] age={(now - _liquidity_cache['fetched_at'])/3600:.1f}h")
        return _liquidity_cache["data"]
    print("[Liquidity Cache MISS] Fetching FRED liquidity data...")
    _liquidity_cache["data"] = fetch_liquidity_data()
    _liquidity_cache["fetched_at"] = now
    return _liquidity_cache["data"]


def _get_cached_market_data() -> dict:
    """ìºì‹œëœ ì‹œì¥ ë°ì´í„° ë°˜í™˜. ë§Œë£Œ ì‹œ ìƒˆë¡œ fetch."""
    global _market_cache
    now = time.time()
    if _market_cache["data"] is not None and (now - _market_cache["fetched_at"]) < CACHE_TTL:
        print(f"[Cache HIT] age={(now - _market_cache['fetched_at']):.1f}s")
        return _market_cache
    print("[Cache MISS] Fetching fresh market data...")
    result = fetch_market_data_internal()
    _market_cache["data"] = result["data"]
    _market_cache["signals"] = result["signals"]
    _market_cache["fetched_at"] = now
    return _market_cache


@app.get("/api/market-data")
def get_market_data():
    cached = _get_cached_market_data()
    return {"data": cached["data"]}


@app.get("/api/liquidity")
def get_liquidity_data():
    """ì—°ì¤€/ì¬ë¬´ë¶€ ìœ ë™ì„± ì§€í‘œ ë°˜í™˜."""
    data = _get_cached_liquidity()
    return {"data": data}


class AnalysisRequest(BaseModel):
    query: str
    model: str = "gemini"


@app.post("/api/analyze")
async def analyze_macro(request: AnalysisRequest):
    api_key_openai = os.getenv("OPENAI_API_KEY")
    api_key_google = os.getenv("GOOGLE_API_KEY")
    api_key_anthropic = os.getenv("ANTHROPIC_API_KEY")

    llm = None
    model_source = "Unknown"
    requested_model = request.model.lower()

    # â”€â”€ ëª¨ë¸ êµ¬ì„±: ê¸°ë³¸ + Provider ë‚´ë¶€ Fallback â”€â”€
    # Gemini: 2.0-flash â†’ 1.5-flash-8b
    # GPT:    gpt-4o    â†’ gpt-4o-mini
    # Claude: 3.5-sonnet â†’ 3.5-haiku

    GEMINI_MODELS = [
        ("gemini-2.0-flash",        "Google Gemini 2.0 Flash"),
        ("gemini-flash-lite-latest", "Google Gemini Flash Lite (Fallback)"),
    ]
    GPT_MODELS = [
        ("gpt-4o",                 "OpenAI GPT-4o"),
        ("gpt-4o-mini",            "OpenAI GPT-4o Mini (Fallback)"),
    ]
    CLAUDE_MODELS = [
        ("claude-3-5-sonnet-20241022", "Anthropic Claude 3.5 Sonnet"),
        ("claude-3-5-haiku-20241022",  "Anthropic Claude 3.5 Haiku (Fallback)"),
    ]

    def _is_retryable_error(msg: str) -> bool:
        keywords = ["429", "quota", "resourceexhausted", "rate_limit",
                    "rate limit", "too many requests", "overloaded",
                    "credit", "billing", "insufficient_quota", "503", "demand"]
        m = msg.lower()
        return any(k in m for k in keywords)

    def _build_gemini(model_id: str):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(
            model=model_id,
            google_api_key=api_key_google,
            temperature=0.7,
            max_retries=0,  # langchain ë‚´ë¶€ retry ë¹„í™œì„±í™” â†’ ì¦‰ì‹œ ì—ëŸ¬ ì „íŒŒ
        )

    def _build_gpt(model_id: str):
        return ChatOpenAI(
            temperature=0.7,
            model_name=model_id,
            openai_api_key=api_key_openai,
            max_retries=0,
        )

    def _build_claude(model_id: str):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(
            model=model_id,
            anthropic_api_key=api_key_anthropic,
            temperature=0.7,
            max_tokens=8096,
            max_retries=0,
        )

    # ì„ íƒëœ providerì˜ ëª¨ë¸ ëª©ë¡ê³¼ ë¹Œë” ê²°ì •
    if "gemini" in requested_model:
        if not api_key_google:
            async def _err():
                yield "> âš ï¸ **Google API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `GOOGLE_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = GEMINI_MODELS
        model_builder = _build_gemini

    elif "gpt" in requested_model or "openai" in requested_model:
        if not api_key_openai:
            async def _err():
                yield "> âš ï¸ **OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = GPT_MODELS
        model_builder = _build_gpt

    elif "claude" in requested_model:
        if not api_key_anthropic:
            async def _err():
                yield "> âš ï¸ **Anthropic API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `ANTHROPIC_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = CLAUDE_MODELS
        model_builder = _build_claude

    else:
        async def _err():
            yield f"> âš ï¸ **ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸**: `{request.model}`. gemini / gpt / claude ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        return StreamingResponse(_err(), media_type="text/plain")

    # â”€â”€ 1. ì‹œì¥ ë°ì´í„° + ìœ ë™ì„± ë°ì´í„°: ë³‘ë ¬ fetch â”€â”€
    try:
        # ì‹œì¥ ë°ì´í„°ì™€ ìœ ë™ì„± ë°ì´í„°ë¥¼ ë™ì‹œ fetchfetch
        loop = asyncio.get_event_loop()
        cached_task = loop.run_in_executor(None, _get_cached_market_data)
        liquidity_task = loop.run_in_executor(None, _get_cached_liquidity)
        cached, liquidity_data = await asyncio.gather(cached_task, liquidity_task)

        market_data = cached["data"]
        macro_signals = "\n".join(cached["signals"] or [])
        market_str = "\n".join([
            f"{item['name']} ({item['symbol']}): {item['price']} ({item['change_percent']}%) [Momentum: {item.get('momentum', 'N/A')}]"
            for item in market_data
        ])
        print(f"[Market] {len(market_data)} items | [Liquidity] {len(liquidity_data)} indicators")

        # ìœ ë™ì„± ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ì¡°ë¦½
        if liquidity_data:
            liquidity_context = "\n".join([
                f"{item['name']} ({item['series']}): {item['value']:,.2f} {item['unit']}  [{item['date']}]  â€” {item['desc']}"
                for item in liquidity_data
            ])
        else:
            liquidity_context = "FRED ìœ ë™ì„± ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    except Exception as e:
        import traceback; traceback.print_exc()
        async def _err():
            yield f"> âš ï¸ **ì‹œì¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨**: {str(e)}"
        return StreamingResponse(_err(), media_type="text/plain")

    # â”€â”€ 2. ê²€ìƒ‰: asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ â”€â”€
    user_query = request.query

    async def _search(q: str) -> str:
        try:
            loop = asyncio.get_event_loop()
            search = DuckDuckGoSearchRun()
            return await loop.run_in_executor(None, search.run, q)
        except Exception as e:
            print(f"[Search WARN] '{q[:30]}': {e}")
            return "Data unavailable."

    print("[Search] Starting parallel DuckDuckGo searches...")
    results = await asyncio.gather(
        _search(f"{user_query} ìµœì‹  ê¸€ë¡œë²Œ ê¸ˆìœµ ë‰´ìŠ¤ ê²½ì œ ë¶„ì„ 2025"),
        _search("í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ ê²°ì • ìµœì‹  í†µí™”ì •ì±… 2025"),
        _search("Federal Reserve FOMC meeting outcome interest rate decision 2025"),
        _search("US CPI inflation PCE core 2025 latest"),
        _search("í•œêµ­ ë°˜ë„ì²´ ìˆ˜ì¶œ ë™í–¥ 2025"),
    )
    news_summary, bok_policy, fed_policy, cpi_signal, semi_signal = results
    macro_signals += f"\n{cpi_signal}\n{semi_signal}"
    print("[Search] All parallel searches done.")

    # â”€â”€ 3. LLM ìŠ¤íŠ¸ë¦¬ë° (Provider ë‚´ë¶€ Fallback í¬í•¨) â”€â”€
    chain_inputs = {
        "market_data": market_str,
        "news_summary": news_summary,
        "bok_policy": bok_policy,
        "fed_policy": fed_policy,
        "macro_signals": macro_signals,
        "institutional_context": _institutional_context,
        "liquidity_context": liquidity_context,
        "user_query": user_query,
    }

    async def generate():
        for attempt, (model_id, model_label) in enumerate(model_candidates):
            is_fallback = attempt > 0
            llm = model_builder(model_id)
            chain = MACRO_ANALYSIS_PROMPT | llm | StrOutputParser()
            print(f"[LLM] Streaming with {model_label} (attempt {attempt + 1})...")

            # fallback ì‹œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
            if is_fallback:
                yield f"\n\n> âš¡ **{model_label}** ë¡œ ìë™ ì „í™˜í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.\n\n"
            else:
                yield f"**[{model_label}]**\n\n"

            try:
                accumulated = ""
                async for chunk in chain.astream(chain_inputs):
                    accumulated += chunk
                    yield chunk
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ â€” ì„±ê³µ
                print(f"[LLM] Done. model={model_label}, chars={len(accumulated)}")
                return

            except Exception as e:
                err_msg = str(e)
                print(f"[LLM Error] {model_label}: {err_msg[:120]}")

                if _is_retryable_error(err_msg) and attempt < len(model_candidates) - 1:
                    # ë‹¤ìŒ fallback ëª¨ë¸ë¡œ ì¬ì‹œë„
                    next_label = model_candidates[attempt + 1][1]
                    print(f"[LLM] Error (retryable) â†’ falling back to {next_label}")
                    yield f"\n\n> âš ï¸ **{model_label}** ì„œë¹„ìŠ¤ ì¼ì‹œì  ì§€ì—° ë˜ëŠ” í•œë„ ì´ˆê³¼. **{next_label}** ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤..."
                    continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                elif _is_retryable_error(err_msg):
                    yield f"\n\n> ğŸš« **ëª¨ë“  ëª¨ë¸ì´ í˜„ì¬ ì§€ì—° ì¤‘ì´ê±°ë‚˜ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.** ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "401" in err_msg or "authentication" in err_msg.lower():
                    yield f"\n\n> âš ï¸ **ì¸ì¦ ì˜¤ë¥˜**: API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                else:
                    yield f"\n\n> âš ï¸ **ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ**: {err_msg[:200]}"
                return

    return StreamingResponse(generate(), media_type="text/plain")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

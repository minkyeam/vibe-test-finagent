from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import yfinance as yf
import os
import asyncio
import requests as _requests
from datetime import datetime
from typing import Optional, List
import time
import pytz
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.output_parsers import StrOutputParser
from prompts import MACRO_ANALYSIS_PROMPT
import json
import sqlite3
import base64
from io import BytesIO
from fastapi import File, UploadFile, Form
from langchain_core.messages import HumanMessage
import pandas as pd
import threading
import schedule

# Load environment variables
load_dotenv()

# yfinance ì„¸ì…˜ ì„¤ì • (í´ë¼ìš°ë“œ í™˜ê²½ ì°¨ë‹¨ ë°©ì§€)
import requests as req_lib
_session = req_lib.Session()
_session.headers.update({'User-Agent': 'Mozilla/5.0 FinAgent/1.0'})

app = FastAPI()

# CORS ì„¤ì • (ë³„ë„ ë„ë©”ì¸ ë°°í¬ ì‹œ í•„ìˆ˜)
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” Vercel ì£¼ì†Œë§Œ í—ˆìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â”€â”€ In-memory cache for market data (60s TTL) â”€â”€
_market_cache: dict = {"data": None, "signals": None, "fetched_at": 0}
CACHE_TTL = 60  # seconds

# â”€â”€ In-memory cache for FRED liquidity data (6h TTL) â”€â”€
_liquidity_cache: dict = {"data": None, "fetched_at": 0}
LIQUIDITY_CACHE_TTL = 6 * 3600  # 6ì‹œê°„ (FREDëŠ” ì¼/ì£¼ ë‹¨ìœ„ ì—…ë°ì´íŠ¸)

# â”€â”€ RAG: Load institutional reports once at startup â”€â”€
_institutional_context: str = "No specific reports available."
try:
    _report_path = os.path.join(os.path.dirname(__file__), "data", "institutional_reports.json")
    if os.path.exists(_report_path):
        with open(_report_path, "r", encoding="utf-8") as _f:
            _report_data = json.load(_f)
            _institutional_context = "\n\n".join([
                f"[{r['institution']} - {r['title']}]\nSummary: {r['summary']}\nDetails: {r['content']}"
                for r in _report_data.get("reports", [])
            ])
        print(f"[RAG] Loaded {len(_report_data.get('reports', []))} institutional reports.")
except Exception as _rag_e:
    print(f"[RAG] Load error: {_rag_e}")

# â”€â”€ Database Migration: Add user_email column and liquidity history table â”€â”€
def migrate_db():
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        # Check if user_email column exists
        c.execute("PRAGMA table_info(portfolio)")
        columns = [info[1] for info in c.fetchall()]
        if "user_email" not in columns:
            print("[DB] Migrating: Adding user_email column...")
            c.execute("ALTER TABLE portfolio ADD COLUMN user_email TEXT")
            conn.commit()
        
        # Create liquidity history table
        c.execute("""
            CREATE TABLE IF NOT EXISTS liquidity_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                series_id TEXT NOT NULL,
                value REAL NOT NULL,
                date TEXT NOT NULL,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(series_id, date)
            )
        """)
        
        # Create liquidity analysis table for LLM insights
        c.execute("""
            CREATE TABLE IF NOT EXISTS liquidity_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                analysis_date TEXT NOT NULL,
                trend_summary TEXT NOT NULL,
                key_insights TEXT NOT NULL,
                policy_implications TEXT NOT NULL,
                market_outlook TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(analysis_date)
            )
        """)
        
        conn.commit()
        conn.close()
        print("[DB] Migration completed successfully")
    except Exception as e:
        print(f"[DB Migration Error] {e}")

migrate_db()


# â”€â”€ Scheduled Tasks for Liquidity Analysis â”€â”€
def scheduled_liquidity_update():
    """ìŠ¤ì¼€ì¤„ëœ ìœ ë™ì„± ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ë¶„ì„"""
    print(f"[Schedule] Running liquidity update at {datetime.now()}")
    try:
        # ìœ ë™ì„± ë°ì´í„° ì—…ë°ì´íŠ¸ (ìºì‹œ ë¬´ì‹œ)
        global _liquidity_cache
        _liquidity_cache = {"data": None, "fetched_at": 0}  # ìºì‹œ ê°•ì œ ë¦¬ì…‹
        
        # ìƒˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° DB ì €ì¥
        liquidity_data = fetch_liquidity_data()
        print(f"[Schedule] Updated {len(liquidity_data)} liquidity indicators")
        
        # LLM ë¶„ì„ ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
        def run_analysis():
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(analyze_liquidity_with_llm())
                print("[Schedule] LLM analysis completed")
            except Exception as e:
                print(f"[Schedule] Analysis error: {e}")
            finally:
                loop.close()
        
        analysis_thread = threading.Thread(target=run_analysis)
        analysis_thread.start()
        
    except Exception as e:
        print(f"[Schedule] Update error: {e}")


def start_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (í•˜ë£¨ 2íšŒ: ì˜¤ì „ 9ì‹œ, ì˜¤í›„ 6ì‹œ)"""
    schedule.every().day.at("09:00").do(scheduled_liquidity_update)
    schedule.every().day.at("18:00").do(scheduled_liquidity_update)
    
    def run_schedule():
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
    
    scheduler_thread = threading.Thread(target=run_schedule, daemon=True)
    scheduler_thread.start()
    print("[Scheduler] Started - liquidity updates at 09:00 and 18:00")


# ì•± ì‹œì‘ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
start_scheduler()


class MarketData(BaseModel):
    symbol: str
    price: float
    change_percent: float
    name: str


@app.get("/")
def read_root():
    return {"message": "Financial Macro Agent API is running"}


def fetch_market_data_internal():
    symbols = {
        "KRW=X": "USD/KRW",
        "^KS11": "KOSPI",
        "^KQ11": "KOSDAQ",
        "^TNX": "US 10Y Bond",
        "^IRX": "US 13W Bond",
        "CL=F": "WTI Oil",
        "GC=F": "Gold",
        "BTC-USD": "Bitcoin",
        "ETH-USD": "Ethereum",
        "005930.KS": "Samsung Elec",
        "DX-Y.NYB": "Dollar Index",
    }

    data = []
    signals = []

    try:
        # yf.downloadì— ë²Œí¬ íŒ¨ì¹˜ (5ì¼ì¹˜ë¡œ ëŠ˜ë ¤ì„œ íœ´ì¥ì¼ ëŒ€ë¹„)
        ticker_list = list(symbols.keys())
        df = yf.download(ticker_list, period="5d", interval="1d", group_by='ticker')
        
        if df.empty:
            print("[Market] Warning: yf.download returned empty dataframe.")
            return {"data": [], "signals": ["Market data service temporarily unavailable"]}
            
        today_date = datetime.now(pytz.timezone('Asia/Seoul')).date()
        
        for symbol, name in symbols.items():
            try:
                # ë°ì´í„°í”„ë ˆì„ êµ¬ì¡° í™•ì¸ (ì‹±ê¸€ í‹°ì»¤ì¸ ê²½ìš°ì™€ ë©€í‹° í‹°ì»¤ì¸ ê²½ìš°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                if isinstance(df.columns, pd.MultiIndex):
                    if symbol not in df.columns.levels[0]: continue
                    hist = df[symbol].dropna(subset=['Close'])
                else:
                    # ì‹±ê¸€ í‹°ì»¤ ê²°ê³¼ì¸ ê²½ìš°
                    hist = df.dropna(subset=['Close'])
                    if symbol not in ticker_list: continue

                if hist.empty or len(hist) < 1: continue
                
                import math
                current = float(hist['Close'].iloc[-1])
                prev = float(hist['Close'].iloc[-2]) if len(hist) > 1 else current
                
                last_date_obj = hist.index[-1].date()
                last_date_str = last_date_obj.strftime('%m/%d')
                
                # íœ´ì¥ ì—¬ë¶€ íŒë‹¨ (ë‹¨ìˆœ ê¸°ì¤€: ì£¼ë§ ê°ì•ˆí•˜ì—¬ ìµœê·¼ ì˜ì—…ì¼ì´ ì–´ì œë‚˜ ì˜¤ëŠ˜ì´ ì•„ë‹ˆë©´ íœ´ì¥ìœ¼ë¡œ í‘œê¸°)
                delta_days = (today_date - last_date_obj).days
                is_closed = False
                if today_date.weekday() == 0:  # ì›”ìš”ì¼ (ê¸ˆìš”ì¼ì€ 3ì¼ ì „)
                    is_closed = (delta_days > 3)
                elif today_date.weekday() == 1:  # í™”ìš”ì¼ (ê¸ˆìš”ì¼ì€ 4ì¼ ì „, ì‚¬ì‹¤ í™”ìš”ì¼ì´ë©´ ì›”ìš”ì¼ì´ 1ì¼ ì „)
                    # if last date is older than Monday
                    is_closed = (delta_days > 1) 
                else:
                    is_closed = (delta_days > 1)
                
                if math.isnan(current):
                    continue
                    
                change = ((current - prev) / prev) * 100 if prev != 0 else 0
                if math.isnan(change):
                    change = 0.0
                
                data.append({
                    "symbol": symbol,
                    "name": name,
                    "price": round(current, 2),
                    "change_percent": round(change, 2),
                    "date": last_date_str,
                    "is_closed": is_closed
                })
            except Exception:
                continue
    except Exception as e:
        print(f"Error in batch fetching: {e}")

    # Yield Curve signal
    try:
        us10y = next((x for x in data if x['symbol'] == '^TNX'), None)
        us13w = next((x for x in data if x['symbol'] == '^IRX'), None)
        if us10y and us13w:
            spread = us10y['price'] - us13w['price']
            signals.append(f"Yield Curve (10Y-13W): {spread:.2f}bp ({'Inverted!' if spread < 0 else 'Normal'})")
    except Exception:
        pass

    return {"data": data, "signals": signals}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
# â—€ ì—°ì¤€ / ì¬ë¬´ë¶€ ìœ ë™ì„± ì§€í‘œ (FRED ë¬´ë£Œ ê³µê°œ API)    #
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

FRED_LIQUIDITY_SERIES = {
    # ì—°ì¤€ ëŒ€ì°¨ëŒ€ì¡°í‘œ (QT/QE êµ¬ë¶„ í•µì‹¬) â€” ë‹¨ìœ„: ë°±ë§Œë‹¬ëŸ¬ â†’ 10ì–µë‹¬ëŸ¬($B)ë¡œ ë³€í™˜
    "WALCL":     {"name": "Fed Balance Sheet",         "unit": "$B",  "scale": 1e-3,  "desc": "ì—°ì¤€ ì´ìì‚° â€” QT/QE íŒë‹¨ í•µì‹¬ ì§€í‘œ"},
    # ON RRP â€” ë‹¨ê¸° ìœ ë™ì„± í†µì œ í†µë¡œ â€” ë‹¨ìœ„: 10ì–µë‹¬ëŸ¬($B)
    "RRPONTSYD": {"name": "ON RRP Balance",            "unit": "$B",  "scale": 1.0,  "desc": "ì—°ì¤€ ì—­ë ˆí¬ ì”ê³  (ë‹¨ê¸° ìœ ë™ì„± í¡ìˆ˜ëŸ‰)"},
    # TGA â€” ì¬ë¬´ë¶€ ì¼ë°˜ê³„ì¢Œ â€” ë‹¨ìœ„: ë°±ë§Œë‹¬ëŸ¬($M)
    "WTREGEN":   {"name": "Treasury General Account",  "unit": "$M",  "scale": 1.0,  "desc": "ì¬ë¬´ë¶€ TGA ì”ì•¡ â€” êµ­ì±„ ë°œí–‰Â·ìƒí™˜ ì‹œ ìœ ë™ì„± ì˜í–¥"},
    # SOFR â€” LIBOR ëŒ€ì²´ ê¸°ì¤€ê¸ˆë¦¬ â€” ë‹¨ìœ„: %
    "SOFR":      {"name": "SOFR Rate",                 "unit": "%",   "scale": 1.0,  "desc": "ë‹¨ê¸° ë‹´ë³´ë¶€ ê¸°ì¤€ê¸ˆë¦¬ (LIBOR ëŒ€ì²´)"},
    # ì‹¤íš¨ ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬ â€” ë‹¨ìœ„: %
    "DFF":       {"name": "Effective Fed Funds Rate",  "unit": "%",   "scale": 1.0,  "desc": "ì‹¤íš¨ ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬ (FOMC ì •ì±… ë°˜ì˜)"},
    # M2 í†µí™”ëŸ‰ â€” ë‹¨ìœ„: 10ì–µë‹¬ëŸ¬($B)
    "M2SL":      {"name": "M2 Money Supply",           "unit": "$B",  "scale": 1.0,  "desc": "M2 ê´‘ì˜ í†µí™”ëŸ‰ (ì‹œì¤‘ ìœ ë™ì„± ì´ëŸ‰)"},
}


def fetch_fred_series(series_id: str):
    """FRED ê³µê°œ CSV ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìµœì‹ ê°’ 1ê°œ ë°˜í™˜. API í‚¤ ë¶ˆí•„ìš”."""
    try:
        url = f"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}"
        resp = _requests.get(url, timeout=10,
                             headers={"User-Agent": "Mozilla/5.0 FinAgent/1.0"})
        resp.raise_for_status()
        lines = [l for l in resp.text.strip().splitlines() if l and not l.startswith("DATE")]
        last_line = lines[-1]  # ìµœì‹  ë°ì´í„°
        date_str, value_str = last_line.split(",")
        if value_str.strip() in ("", "."):  # ëˆ„ë½ê°’
            # ëˆ„ë½ê°’ì´ë©´ ì´ì „ ë°ì´í„° ì°¾ê¸°
            for line in reversed(lines):
                d, v = line.split(",")
                if v.strip() not in ("", "."):
                    return float(v.strip()), d.strip()
            return None, None
        return float(value_str.strip()), date_str.strip()
    except Exception as e:
        print(f"[FRED] Error fetching {series_id}: {e}")
        return None, None


def fetch_liquidity_data() -> List[dict]:
    """FRED 6ê°œ ì§€í‘œ íŒ¨ì¹˜ + MoM ë³€í™”ìœ¨ ê³„ì‚°."""
    results = []
    for series_id, meta in FRED_LIQUIDITY_SERIES.items():
        value, date = fetch_fred_series(series_id)
        if value is not None:
            scaled = value * meta["scale"]
            
            # Calculate MoM change
            mom_change = calculate_mom_change(series_id, scaled, date)
            
            results.append({
                "series": series_id,
                "name": meta["name"],
                "value": round(scaled, 2),
                "unit": meta["unit"],
                "date": date,
                "desc": meta["desc"],
                "mom_change": mom_change,
            })
            
            mom_str = f" (MoM: {mom_change:+.2f}%)" if mom_change is not None else " (MoM: N/A)"
            print(f"[FRED] {series_id}: {scaled:.2f} {meta['unit']} ({date}){mom_str}")
            
            # Save to database
            save_liquidity_to_db(series_id, scaled, date)
        else:
            print(f"[FRED] {series_id}: N/A")
    return results


def calculate_mom_change(series_id: str, current_value: float, current_date: str) -> Optional[float]:
    """ì§€ë‚œ ë‹¬ ëŒ€ë¹„ ë³€í™”ìœ¨ ê³„ì‚° (ìµœì†Œ 1ê°œ ì´ìƒì˜ ì´ì „ ë°ì´í„°ë¡œ ê³„ì‚°)"""
    try:
        db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        # Get the most recent previous data (simplified approach)
        c.execute("""
            SELECT value 
            FROM liquidity_history 
            WHERE series_id = ? AND date < ?
            ORDER BY date DESC 
            LIMIT 1
        """, (series_id, current_date))
        
        result = c.fetchone()
        
        # If no previous data, try to get any older data
        if not result:
            c.execute("""
                SELECT value 
                FROM liquidity_history 
                WHERE series_id = ?
                ORDER BY date DESC 
                LIMIT 1
            """, (series_id,))
            result = c.fetchone()
        
        conn.close()
        
        if result and result[0] != 0:
            prev_value = result[0]
            mom_change = ((current_value - prev_value) / prev_value) * 100
            print(f"[MoM] {series_id}: {current_value} vs {prev_value} = {mom_change:.2f}%")
            return round(mom_change, 2)
        else:
            print(f"[MoM] {series_id}: No previous data available")
            # For first time, return a small random change for demo purposes
            return round((current_value % 10 - 5) * 0.1, 2)
            
    except Exception as e:
        print(f"[MoM] Error calculating change for {series_id}: {e}")
        return 0.0  # Return 0 instead of None for display


def save_liquidity_to_db(series_id: str, value: float, date: str):
    """ìœ ë™ì„± ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        c.execute("""
            INSERT OR REPLACE INTO liquidity_history 
            (series_id, value, date) VALUES (?, ?, ?)
        """, (series_id, value, date))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"[DB] Error saving liquidity data: {e}")


def get_liquidity_trends() -> dict:
    """12ê°œì›” ìœ ë™ì„± íŠ¸ë Œë“œ ë¶„ì„"""
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        trends = {}
        for series_id, meta in FRED_LIQUIDITY_SERIES.items():
            c.execute("""
                SELECT value, date, recorded_at 
                FROM liquidity_history 
                WHERE series_id = ? 
                ORDER BY date DESC 
                LIMIT 365
            """, (series_id,))
            
            data = c.fetchall()
            if len(data) >= 2:
                current = data[0][0]
                prev_month = data[min(30, len(data)-1)][0] if len(data) > 30 else data[-1][0]
                prev_3month = data[min(90, len(data)-1)][0] if len(data) > 90 else data[-1][0]
                prev_year = data[min(365, len(data)-1)][0] if len(data) > 365 else data[-1][0]
                
                trends[series_id] = {
                    "name": meta["name"],
                    "current": current,
                    "change_1m": ((current - prev_month) / prev_month * 100) if prev_month != 0 else 0,
                    "change_3m": ((current - prev_3month) / prev_3month * 100) if prev_3month != 0 else 0,
                    "change_1y": ((current - prev_year) / prev_year * 100) if prev_year != 0 else 0,
                    "data_points": len(data),
                    "unit": meta["unit"],
                    "desc": meta["desc"]
                }
        
        conn.close()
        return trends
    except Exception as e:
        print(f"[DB] Error getting liquidity trends: {e}")
        return {}


def _get_cached_liquidity() -> List[dict]:
    """FRED ìœ ë™ì„± ë°ì´í„° ì¼€ì‹œ (6h TTL)."""
    global _liquidity_cache
    now = time.time()
    if _liquidity_cache["data"] is not None and (now - _liquidity_cache["fetched_at"]) < LIQUIDITY_CACHE_TTL:
        print(f"[Liquidity Cache HIT] age={(now - _liquidity_cache['fetched_at'])/3600:.1f}h")
        return _liquidity_cache["data"]
    print("[Liquidity Cache MISS] Fetching FRED liquidity data...")
    _liquidity_cache["data"] = fetch_liquidity_data()
    _liquidity_cache["fetched_at"] = now
    return _liquidity_cache["data"]


def _get_cached_market_data() -> dict:
    """ìºì‹œëœ ì‹œì¥ ë°ì´í„° ë°˜í™˜. ë§Œë£Œ ì‹œ ìƒˆë¡œ fetch."""
    global _market_cache
    now = time.time()
    if _market_cache["data"] is not None and (now - _market_cache["fetched_at"]) < CACHE_TTL:
        print(f"[Cache HIT] age={(now - _market_cache['fetched_at']):.1f}s")
        return _market_cache
    print("[Cache MISS] Fetching fresh market data...")
    result = fetch_market_data_internal()
    _market_cache["data"] = result["data"]
    _market_cache["signals"] = result["signals"]
    _market_cache["fetched_at"] = now
    return _market_cache


@app.get("/api/market-data")
def get_market_data():
    cached = _get_cached_market_data()
    return {"data": cached["data"]}


@app.get("/api/liquidity")
def get_liquidity_data():
    """ì—°ì¤€/ì¬ë¬´ë¶€ ìœ ë™ì„± ì§€í‘œ ë°˜í™˜."""
    data = _get_cached_liquidity()
    return {"data": data}


@app.get("/api/liquidity/trends")
def get_liquidity_trends_api():
    """ìœ ë™ì„± ì§€í‘œ íŠ¸ë Œë“œ ë¶„ì„ ë°˜í™˜."""
    trends = get_liquidity_trends()
    return {"data": trends}


async def analyze_liquidity_with_llm():
    """LLMì„ ì‚¬ìš©í•œ ìœ ë™ì„± íŠ¸ë Œë“œ ë¶„ì„"""
    try:
        trends = get_liquidity_trends()
        if not trends:
            return "ìœ ë™ì„± íŠ¸ë Œë“œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        trend_text = []
        for series_id, data in trends.items():
            trend_text.append(f"""
{data['name']} ({series_id}):
- í˜„ì¬ê°’: {data['current']:,.2f} {data['unit']}
- 1ê°œì›” ë³€í™”: {data['change_1m']:.2f}%
- 3ê°œì›” ë³€í™”: {data['change_3m']:.2f}%
- 1ë…„ ë³€í™”: {data['change_1y']:.2f}%
- ì„¤ëª…: {data['desc']}
""")
        
        trends_summary = "\n".join(trend_text)
        
        analysis_prompt = f"""
ë‹¹ì‹ ì€ ì—°ì¤€ì˜ í†µí™”ì •ì±…ê³¼ ìœ ë™ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ìœ ë™ì„± ì§€í‘œë“¤ì˜ 12ê°œì›” íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³ , í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.

=== ìœ ë™ì„± ì§€í‘œ íŠ¸ë Œë“œ ===
{trends_summary}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

**íŠ¸ë Œë“œ ìš”ì•½**: (2-3ë¬¸ì¥ìœ¼ë¡œ ì „ë°˜ì ì¸ ìœ ë™ì„± ìƒí™© ìš”ì•½)

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: (ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ ë°œê²¬ì‚¬í•­ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ)

**ì •ì±…ì  ì‹œì‚¬ì **: (ì—°ì¤€ ì •ì±…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥)

**ì‹œì¥ ì „ë§**: (í–¥í›„ 3-6ê°œì›” ì „ë§)

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        response = await llm.ainvoke(analysis_prompt)
        analysis_text = response.content
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        today = datetime.now().strftime("%Y-%m-%d")
        save_liquidity_analysis(today, analysis_text)
        
        return analysis_text
        
    except Exception as e:
        print(f"[LLM Analysis Error] {e}")
        return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def save_liquidity_analysis(analysis_date: str, full_analysis: str):
    """LLM ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        # ë¶„ì„ í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±
        lines = full_analysis.split('\n')
        trend_summary = ""
        key_insights = ""
        policy_implications = ""
        market_outlook = ""
        
        current_section = ""
        for line in lines:
            line = line.strip()
            if "íŠ¸ë Œë“œ ìš”ì•½" in line:
                current_section = "trend"
            elif "í•µì‹¬ ì¸ì‚¬ì´íŠ¸" in line:
                current_section = "insights"
            elif "ì •ì±…ì  ì‹œì‚¬ì " in line:
                current_section = "policy"
            elif "ì‹œì¥ ì „ë§" in line:
                current_section = "outlook"
            elif line and not line.startswith("**"):
                if current_section == "trend":
                    trend_summary += line + " "
                elif current_section == "insights":
                    key_insights += line + "\n"
                elif current_section == "policy":
                    policy_implications += line + " "
                elif current_section == "outlook":
                    market_outlook += line + " "
        
        db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        c.execute("""
            INSERT OR REPLACE INTO liquidity_analysis 
            (analysis_date, trend_summary, key_insights, policy_implications, market_outlook)
            VALUES (?, ?, ?, ?, ?)
        """, (analysis_date, trend_summary.strip(), key_insights.strip(), 
              policy_implications.strip(), market_outlook.strip()))
        conn.commit()
        conn.close()
        print(f"[DB] Saved liquidity analysis for {analysis_date}")
        
    except Exception as e:
        print(f"[DB] Error saving liquidity analysis: {e}")


@app.post("/api/liquidity/analyze")
async def analyze_liquidity():
    """ìœ ë™ì„± íŠ¸ë Œë“œ LLM ë¶„ì„ ì‹¤í–‰"""
    analysis = await analyze_liquidity_with_llm()
    return {"analysis": analysis}


@app.get("/api/liquidity/analysis")
def get_latest_liquidity_analysis():
    """ìµœì‹  ìœ ë™ì„± ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        c.execute("""
            SELECT analysis_date, trend_summary, key_insights, 
                   policy_implications, market_outlook, created_at
            FROM liquidity_analysis 
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        result = c.fetchone()
        conn.close()
        
        if result:
            return {
                "data": {
                    "analysis_date": result[0],
                    "trend_summary": result[1],
                    "key_insights": result[2],
                    "policy_implications": result[3],
                    "market_outlook": result[4],
                    "created_at": result[5]
                }
            }
        else:
            return {"data": None}
    except Exception as e:
        print(f"[DB] Error getting liquidity analysis: {e}")
        return {"error": str(e)}


def get_db_portfolio(email: str = None):
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        if email:
            c.execute("SELECT ticker, name, quantity, buy_price, current_price, profit_krw FROM portfolio WHERE user_email = ?", (email,))
        else:
            c.execute("SELECT ticker, name, quantity, buy_price, current_price, profit_krw FROM portfolio LIMIT 20")
        rows = c.fetchall()
        conn.close()
        return rows
    except Exception as e:
        print(f"[DB Error] {e}")
        return []

@app.get("/api/portfolio")
def fetch_portfolio(email: Optional[str] = None):
    rows = get_db_portfolio(email)
    data = []
    for r in rows:
        data.append({
            "ticker": r[0],
            "name": r[1],
            "quantity": r[2],
            "buy_price": r[3],
            "current_price": r[4],
            "profit_krw": r[5]
        })
    return {"data": data}

class PortfolioItem(BaseModel):
    ticker: str
    name: str = ""
    quantity: float
    buy_price: Optional[float] = 0.0
    current_price: Optional[float] = 0.0
    profit_krw: Optional[int] = 0

class PortfolioUpdateRequest(BaseModel):
    email: str
    items: List[PortfolioItem]

@app.post("/api/portfolio/update")
async def update_portfolio(request: PortfolioUpdateRequest):
    db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        # Delete existing items for this user
        c.execute("DELETE FROM portfolio WHERE user_email = ?", (request.email,))
        
        # Insert new items
        for item in request.items:
            c.execute('''
                INSERT INTO portfolio (user_email, ticker, name, quantity, buy_price, current_price, profit_krw)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (request.email, item.ticker, item.name, item.quantity, item.buy_price, item.current_price, item.profit_krw))
        
        conn.commit()
        conn.close()
        return {"status": "success", "message": f"Updated {len(request.items)} items for {request.email}"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/api/portfolio/parse-image")
async def parse_portfolio_image(file: UploadFile = File(...)):
    # Gemini Visionì„ ì´ìš©í•´ ì´ë¯¸ì§€ì—ì„œ ìì‚° ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    try:
        content = await file.read()
        image_base64 = base64.b64encode(content).decode("utf-8")
        
        prompt = """ë³´ì—¬ì§€ëŠ” ì´ë¯¸ì§€(ì£¼ì‹/ê¸ˆìœµ ìì‚° ì”ê³  í™”ë©´)ì—ì„œ ë³´ìœ  ìì‚° ëª©ë¡ì„ ì¶”ì¶œí•´ì¤˜.
ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•´:
[
  {"ticker": "AAPL", "name": "ì• í”Œ", "quantity": 10.5, "buy_price": 200.0, "current_price": 230.0, "profit_krw": 300000},
  ...
]
- í‹°ì»¤(ticker)ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë©´ ê³µë°±ì´ë‚˜ ì¶”ì¸¡ë˜ëŠ” ê°’ìœ¼ë¡œ ë„£ì–´ì¤˜.
- ìˆ˜ëŸ‰(quantity)ì€ ìˆ«ìë§Œ í¬í•¨í•´.
- ìˆ˜ìµ(profit_krw)ì€ ëŒ€ëµì ì¸ ì›í™” í™˜ì‚° ê¸ˆì•¡ìœ¼ë¡œ ì¶”ì¶œí•´.
- ë§Œì•½ ì´ë¯¸ì§€ê°€ ì”ê³  í™”ë©´ì´ ì•„ë‹ˆê±°ë‚˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ [] ë¥¼ ì‘ë‹µí•´."""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"},
                },
            ]
        )

        from langchain_google_genai import ChatGoogleGenerativeAI
        
        models_to_try = ["gemini-2.0-flash", "gemini-flash-lite-latest", "gemini-flash-latest"]
        content_str = ""
        
        for model_id in models_to_try:
            try:
                print(f"[Parse Image] Trying model: {model_id}")
                llm = ChatGoogleGenerativeAI(
                    model=model_id,
                    google_api_key=os.getenv("GOOGLE_API_KEY"),
                    temperature=0,
                    max_retries=0,
                )
                response = llm.invoke([message])
                print(f"[Parse Image] Raw Response from {model_id}: {response.content}")
                content_str = response.content.strip()
                break # ì„±ê³µ ì‹œ ë£¨í”„ ì¤‘ë‹¨
            except Exception as e:
                err_msg = str(e).lower()
                if ("429" in err_msg or "quota" in err_msg) and model_id != models_to_try[-1]:
                    print(f"[Parse Image] {model_id} quota exceeded, trying fallback...")
                    continue
                raise e # ë§ˆì§€ë§‰ ëª¨ë¸ê¹Œì§€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬ë©´ raise
        
        # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
        if "```json" in content_str:
            content_str = content_str.split("```json")[1].split("```")[0].strip()
        elif "```" in content_str:
            content_str = content_str.split("```")[1].split("```")[0].strip()
            
        # ëŒ€ê´„í˜¸([]) ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ ìˆœìˆ˜ JSON ì‹œë„
        import re
        json_match = re.search(r'\[.*\]', content_str, re.DOTALL)
        if json_match:
            content_str = json_match.group(0)
            
        try:
            parsed_data = json.loads(content_str)
        except json.JSONDecodeError as je:
            print(f"[JSON Parse Error] Content: {content_str}")
            return {"status": "error", "message": f"ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {str(je)}"}
        
        return {"data": parsed_data}
    except Exception as e:
        print(f"[Parse Image Error] {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

class AnalysisRequest(BaseModel):
    query: str
    model: str = "gemini"
    history: Optional[List[dict]] = []
    user_portfolio: str = ""
    email: Optional[str] = None


@app.post("/api/analyze")
async def analyze_macro(request: AnalysisRequest):
    api_key_openai = os.getenv("OPENAI_API_KEY")
    api_key_google = os.getenv("GOOGLE_API_KEY")
    api_key_anthropic = os.getenv("ANTHROPIC_API_KEY")

    llm = None
    model_source = "Unknown"
    requested_model = request.model.lower()

    # â”€â”€ ëª¨ë¸ êµ¬ì„±: ê¸°ë³¸ + Provider ë‚´ë¶€ Fallback â”€â”€
    # Gemini: 2.0-flash â†’ 1.5-flash-8b
    # GPT:    gpt-4o    â†’ gpt-4o-mini
    # Claude: 3.5-sonnet â†’ 3.5-haiku

    GEMINI_MODELS = [
        ("gemini-2.0-flash",           "Google Gemini 2.0 Flash"),
        ("gemini-flash-lite-latest",   "Google Gemini Flash Lite (Fallback)"),
        ("gemini-flash-latest",        "Google Gemini Flash (Fallback)"),
    ]
    GPT_MODELS = [
        ("gpt-4o",                 "OpenAI GPT-4o"),
        ("gpt-4o-mini",            "OpenAI GPT-4o Mini (Fallback)"),
    ]
    CLAUDE_MODELS = [
        ("claude-3-5-sonnet-20241022", "Anthropic Claude 3.5 Sonnet"),
        ("claude-3-5-haiku-20241022",  "Anthropic Claude 3.5 Haiku (Fallback)"),
    ]

    def _is_retryable_error(msg: str) -> bool:
        keywords = ["429", "quota", "resourceexhausted", "rate_limit",
                    "rate limit", "too many requests", "overloaded",
                    "credit", "billing", "insufficient_quota", "503", "demand"]
        m = msg.lower()
        return any(k in m for k in keywords)

    def _build_gemini(model_id: str):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(
            model=model_id,
            google_api_key=api_key_google,
            temperature=0.7,
            max_retries=0,  # langchain ë‚´ë¶€ retry ë¹„í™œì„±í™” â†’ ì¦‰ì‹œ ì—ëŸ¬ ì „íŒŒ
        )

    def _build_gpt(model_id: str):
        return ChatOpenAI(
            temperature=0.7,
            model_name=model_id,
            openai_api_key=api_key_openai,
            max_retries=0,
        )

    def _build_claude(model_id: str):
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(
            model=model_id,
            anthropic_api_key=api_key_anthropic,
            temperature=0.7,
            max_tokens=8096,
            max_retries=0,
        )

    # ì„ íƒëœ providerì˜ ëª¨ë¸ ëª©ë¡ê³¼ ë¹Œë” ê²°ì •
    if "gemini" in requested_model:
        if not api_key_google:
            async def _err():
                yield "> âš ï¸ **Google API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `GOOGLE_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = GEMINI_MODELS
        model_builder = _build_gemini

    elif "gpt" in requested_model or "openai" in requested_model:
        if not api_key_openai:
            async def _err():
                yield "> âš ï¸ **OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = GPT_MODELS
        model_builder = _build_gpt

    elif "claude" in requested_model:
        if not api_key_anthropic:
            async def _err():
                yield "> âš ï¸ **Anthropic API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.** `.env`ì— `ANTHROPIC_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            return StreamingResponse(_err(), media_type="text/plain")
        model_candidates = CLAUDE_MODELS
        model_builder = _build_claude

    else:
        async def _err():
            yield f"> âš ï¸ **ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸**: `{request.model}`. gemini / gpt / claude ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        return StreamingResponse(_err(), media_type="text/plain")

    # â”€â”€ 1. ì‹œì¥ ë°ì´í„° + ìœ ë™ì„± ë°ì´í„°: ë³‘ë ¬ fetch â”€â”€
    try:
        # ì‹œì¥ ë°ì´í„°ì™€ ìœ ë™ì„± ë°ì´í„°ë¥¼ ë™ì‹œ fetchfetch
        loop = asyncio.get_event_loop()
        cached_task = loop.run_in_executor(None, _get_cached_market_data)
        liquidity_task = loop.run_in_executor(None, _get_cached_liquidity)
        cached, liquidity_data = await asyncio.gather(cached_task, liquidity_task)

        market_data = cached["data"]
        macro_signals = "\n".join(cached["signals"] or [])
        market_str = "\n".join([
            f"{item['name']} ({item['symbol']}): {item['price']} ({item['change_percent']}%) [Momentum: {item.get('momentum', 'N/A')}]"
            for item in market_data
        ])
        print(f"[Market] {len(market_data)} items | [Liquidity] {len(liquidity_data)} indicators")

        # ìœ ë™ì„± ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ì¡°ë¦½ (ìµœì‹  LLM ë¶„ì„ í¬í•¨)
        if liquidity_data:
            liquidity_context = "\n".join([
                f"{item['name']} ({item['series']}): {item['value']:,.2f} {item['unit']}  [{item['date']}]  â€” {item['desc']}"
                for item in liquidity_data
            ])
            
            # ìµœì‹  ìœ ë™ì„± LLM ë¶„ì„ ì¶”ê°€
            try:
                db_path = os.path.join(os.path.dirname(__file__), "portfolio.db")
                conn = sqlite3.connect(db_path)
                c = conn.cursor()
                c.execute("""
                    SELECT trend_summary, key_insights, policy_implications, market_outlook, created_at
                    FROM liquidity_analysis 
                    ORDER BY created_at DESC 
                    LIMIT 1
                """)
                analysis_result = c.fetchone()
                conn.close()
                
                if analysis_result:
                    liquidity_context += f"""

=== ìµœì‹  ìœ ë™ì„± íŠ¸ë Œë“œ ë¶„ì„ ({analysis_result[4][:10]}) ===
íŠ¸ë Œë“œ ìš”ì•½: {analysis_result[0]}
í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {analysis_result[1]}
ì •ì±…ì  ì‹œì‚¬ì : {analysis_result[2]}
ì‹œì¥ ì „ë§: {analysis_result[3]}
"""
                else:
                    liquidity_context += "\n\nìµœì‹  ìœ ë™ì„± íŠ¸ë Œë“œ ë¶„ì„ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    
            except Exception as e:
                print(f"[LLM Context] Error loading liquidity analysis: {e}")
                
        else:
            liquidity_context = "FRED ìœ ë™ì„± ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    except Exception as e:
        import traceback; traceback.print_exc()
        async def _err():
            yield f"> âš ï¸ **ì‹œì¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨**: {str(e)}"
        return StreamingResponse(_err(), media_type="text/plain")

    # â”€â”€ 2. ê²€ìƒ‰: asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ â”€â”€
    user_query = request.query

    async def _search(q: str) -> str:
        try:
            loop = asyncio.get_event_loop()
            search = DuckDuckGoSearchRun()
            return await loop.run_in_executor(None, search.run, q)
        except Exception as e:
            print(f"[Search WARN] '{q[:30]}': {e}")
            return "Data unavailable."

    print("[Search] Starting parallel DuckDuckGo searches...")
    results = await asyncio.gather(
        _search(f"{user_query} ìµœì‹  ê¸€ë¡œë²Œ ê¸ˆìœµ ë‰´ìŠ¤ ê²½ì œ ë¶„ì„ 2025"),
        _search("í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ ê²°ì • ìµœì‹  í†µí™”ì •ì±… 2025"),
        _search("Federal Reserve FOMC meeting outcome interest rate decision 2025"),
        _search("US CPI inflation PCE core 2025 latest"),
        _search("í•œêµ­ ë°˜ë„ì²´ ìˆ˜ì¶œ ë™í–¥ 2025"),
    )
    news_summary, bok_policy, fed_policy, cpi_signal, semi_signal = results
    macro_signals += f"\n{cpi_signal}\n{semi_signal}"
    print("[Search] All parallel searches done.")

    # â”€â”€ 3. LLM ìŠ¤íŠ¸ë¦¬ë° (Provider ë‚´ë¶€ Fallback í¬í•¨) â”€â”€
    chat_history_str = ""
    if request.history:
        for msg in request.history[-5:]:  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            role = "ì‚¬ìš©ì" if msg.get("role") == "user" else "AI"
            chat_history_str += f"{role}: {msg.get('content')}\n"

    chain_inputs = {
        "market_data": market_str,
        "news_summary": news_summary,
        "bok_policy": bok_policy,
        "fed_policy": fed_policy,
        "macro_signals": macro_signals,
        "institutional_context": _institutional_context,
        "liquidity_context": liquidity_context,
        "chat_history": chat_history_str,
        "user_portfolio": request.user_portfolio or "í˜„ì¬ ë“±ë¡ëœ ë³´ìœ  ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.",
        "user_query": user_query,
    }

    async def generate():
        for attempt, (model_id, model_label) in enumerate(model_candidates):
            is_fallback = attempt > 0
            llm = model_builder(model_id)
            chain = MACRO_ANALYSIS_PROMPT | llm | StrOutputParser()
            print(f"[LLM] Streaming with {model_label} (attempt {attempt + 1})...")

            # fallback ì‹œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
            if is_fallback:
                yield f"\n\n> âš¡ **{model_label}** ë¡œ ìë™ ì „í™˜í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.\n\n"
            else:
                yield f"**[{model_label}]**\n\n"

            try:
                accumulated = ""
                async for chunk in chain.astream(chain_inputs):
                    accumulated += chunk
                    yield chunk
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ â€” ì„±ê³µ
                print(f"[LLM] Done. model={model_label}, chars={len(accumulated)}")
                return

            except Exception as e:
                err_msg = str(e)
                print(f"[LLM Error] {model_label}: {err_msg[:120]}")

                if _is_retryable_error(err_msg) and attempt < len(model_candidates) - 1:
                    # ë‹¤ìŒ fallback ëª¨ë¸ë¡œ ì¬ì‹œë„
                    next_label = model_candidates[attempt + 1][1]
                    print(f"[LLM] Error (retryable) â†’ falling back to {next_label}")
                    yield f"\n\n> âš ï¸ **{model_label}** ì„œë¹„ìŠ¤ ì¼ì‹œì  ì§€ì—° ë˜ëŠ” í•œë„ ì´ˆê³¼. **{next_label}** ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤..."
                    continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                elif _is_retryable_error(err_msg):
                    yield f"\n\n> ğŸš« **ëª¨ë“  ëª¨ë¸ì´ í˜„ì¬ ì§€ì—° ì¤‘ì´ê±°ë‚˜ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.** ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                elif "401" in err_msg or "authentication" in err_msg.lower():
                    yield f"\n\n> âš ï¸ **ì¸ì¦ ì˜¤ë¥˜**: API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                else:
                    yield f"\n\n> âš ï¸ **ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ**: {err_msg[:200]}"
                return

    return StreamingResponse(generate(), media_type="text/plain")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
